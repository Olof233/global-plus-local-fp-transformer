{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb35364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle as pkl\n",
    "import pprint\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from torch.utils.collect_env import get_pretty_env_info\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import _init_paths\n",
    "from config import config\n",
    "from config import update_config\n",
    "from core.function import inference, test\n",
    "from core.loss import build_criterion\n",
    "from dataset import build_dataloader\n",
    "from dataset import RealLabelsImagenet\n",
    "from models import build_model\n",
    "from utils.comm import comm\n",
    "from utils.utils import create_logger\n",
    "from utils.utils import init_distributed\n",
    "from utils.utils import setup_cudnn\n",
    "from utils.utils import summary_model_on_master\n",
    "from utils.utils import strip_prefix_if_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94dc01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        default=r'D:\\desktop\\1\\z\\feature\\transformer\\original-global-plus-local-fp-transformer\\experiments\\global_plus_local-inference.yaml' ,\n",
    "                        type=str)\n",
    "\n",
    "    # distributed training\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--port\", type=int, default=9000)\n",
    "\n",
    "parser.add_argument('opts',\n",
    "                    help=\"Modify config options using the command-line\",\n",
    "                    default=None,\n",
    "                    nargs=argparse.REMAINDER)\n",
    "\n",
    "args = parser.parse_args(args=['--cfg','D:/desktop/1\\z/feature/transformer/original-global-plus-local-fp-transformer/experiments/global_plus_local-inference.yaml',\n",
    "                               '--local_rank','0', '--port','9000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6cc0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 04:47:55,749:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,749:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,749:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,757:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,757:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,757:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,760:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,760:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,760:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,765:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,765:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,765:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,770:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,770:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,770:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,776:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,776:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,776:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,781:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,781:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,781:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,786:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,786:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,786:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,791:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,791:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,791:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,795:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,795:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,795:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,798:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,798:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,798:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,802:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,802:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,802:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,815:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,815:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,815:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,820:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,820:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,820:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,824:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,824:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,824:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,828:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,828:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,828:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,830:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,830:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,830:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,833:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,833:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,833:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,835:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,835:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,835:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,839:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,839:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,839:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,842:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,842:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,842:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,846:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,846:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,846:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,849:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,849:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,849:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,853:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,853:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,853:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,855:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,855:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,855:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,858:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,858:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,858:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,861:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,861:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,861:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,864:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,864:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,864:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,866:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,866:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,866:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,868:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,868:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,868:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,870:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,870:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,870:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,874:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,874:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,874:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,876:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,876:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,876:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,880:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,880:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,880:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,882:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,882:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,882:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:55,886:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,886:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:55,886:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from D:/desktop/1\\z/feature/transformer/original-global-plus-local-fp-transformer/experiments/global_plus_local-inference.yaml\n",
      "=> creating D:\\desktop\\1\\z\\SOCOFing\\embs ...\n",
      "=> creating D:\\desktop\\1\\z\\SOCOFing\\embs\\imagenet\\global_plus_local-inferenceglobal_plus_local-inferenceglobal_plus_local-inference ...\n",
      "=> setup logger ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 04:47:56,023:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,023:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,023:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,027:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,027:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,027:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,029:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,029:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,029:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,033:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,033:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,033:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,036:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,036:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,036:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,039:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,039:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,039:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,044:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,044:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,044:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,047:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,047:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,047:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,052:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,052:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,052:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,058:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,058:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,058:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,062:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,062:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,062:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,067:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,067:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,067:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,072:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,072:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,072:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,076:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,076:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,076:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,079:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,079:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,079:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,083:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,083:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,083:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,086:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,086:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,086:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,090:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,090:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,090:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,093:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,093:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,093:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,097:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,097:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,097:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,101:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,101:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,101:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,109:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,109:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,109:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,112:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,112:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,112:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,121:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,121:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,121:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,125:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,125:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,125:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,129:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,129:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,129:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,132:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,132:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,132:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,135:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,135:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,135:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,138:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,138:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,138:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,142:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,142:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,142:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,146:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,146:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,146:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,149:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,149:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,149:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,152:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,152:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,152:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,158:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,158:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,158:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,162:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,162:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,162:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,168:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,168:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,168:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,173:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,173:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,173:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,177:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,177:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,177:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,181:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,181:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,181:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,185:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,185:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,185:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,188:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,188:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,188:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,192:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,192:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,192:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,195:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,195:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,195:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,199:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,199:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,199:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,202:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,202:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,202:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,209:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,209:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,209:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,212:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,212:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,212:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,219:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,219:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,219:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,224:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,224:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,224:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,228:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,228:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,228:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,231:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,231:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,231:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,237:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,237:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,237:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,241:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,241:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,241:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,244:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,244:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,244:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,246:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,246:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,246:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,251:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,251:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,251:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,254:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,254:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,254:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,260:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,260:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,260:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,263:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,263:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,263:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,270:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,270:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,270:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,275:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,275:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,275:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,279:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,279:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,279:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,283:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,283:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,283:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,287:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,287:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,287:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,290:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,290:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,290:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,294:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,294:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,294:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,298:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,298:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,298:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,302:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,302:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,302:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,304:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,304:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,304:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,310:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,310:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,310:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,314:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,314:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,314:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,320:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,320:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,320:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,330:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,330:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,330:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,337:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,337:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,337:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,344:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,344:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,344:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,349:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,349:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,349:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,352:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,352:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,352:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,359:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,359:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,359:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,362:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,362:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,362:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,367:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,367:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,367:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,371:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,371:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,371:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,379:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,379:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,379:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,383:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,383:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,383:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,393:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,393:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,393:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,397:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,397:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,397:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,403:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,403:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,403:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,406:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,406:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,406:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,410:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,410:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,410:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,414:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,414:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,414:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,417:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,417:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,417:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,421:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,421:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,421:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,426:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,426:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,426:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,429:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,429:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,429:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,436:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,436:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,436:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,440:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,440:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,440:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,445:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,445:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,445:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,450:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,450:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,450:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,455:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,455:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,455:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,458:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,458:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,458:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,463:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,463:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,463:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,466:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,466:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,466:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,471:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,471:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,471:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,474:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,474:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,474:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,478:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,478:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,478:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,482:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,482:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,482:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,489:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,489:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,489:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,492:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,492:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,492:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,499:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,499:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,499:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,503:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,503:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,503:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,507:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,507:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,507:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,510:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,510:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,510:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,515:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,515:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,515:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,517:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,517:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,517:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,522:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,522:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,522:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,524:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,524:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,524:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,528:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,528:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,528:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,531:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,531:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,531:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,541:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,541:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,541:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,546:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,546:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,546:[P:8712]:Rank[0/1] => init weight of Linear from trunc norm\n",
      "2025-04-12 04:47:56,557:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,557:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,557:[P:8712]:Rank[0/1] => init bias of Linear to zeros\n",
      "2025-04-12 04:47:56,978:[P:8712]:Rank[0/1] => CVTDETR(\n",
      "  (cvt): ConvolutionalVisionTransformerWithTokenOut(\n",
      "    (stage0): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage1): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage2): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.2, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (head): Linear(in_features=384, out_features=192, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionEmbeddingLearned(\n",
      "    (row_embed): Embedding(50, 192)\n",
      "    (col_embed): Embedding(50, 192)\n",
      "  )\n",
      "  (query_encoder): Embedding(50, 384)\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (kd_linear): Linear(in_features=384, out_features=64, bias=True)\n",
      "  (posori_mlp): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (1): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (2): Linear(in_features=384, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-04-12 04:47:56,978:[P:8712]:Rank[0/1] => CVTDETR(\n",
      "  (cvt): ConvolutionalVisionTransformerWithTokenOut(\n",
      "    (stage0): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage1): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage2): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.2, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (head): Linear(in_features=384, out_features=192, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionEmbeddingLearned(\n",
      "    (row_embed): Embedding(50, 192)\n",
      "    (col_embed): Embedding(50, 192)\n",
      "  )\n",
      "  (query_encoder): Embedding(50, 384)\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (kd_linear): Linear(in_features=384, out_features=64, bias=True)\n",
      "  (posori_mlp): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (1): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (2): Linear(in_features=384, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-04-12 04:47:56,978:[P:8712]:Rank[0/1] => CVTDETR(\n",
      "  (cvt): ConvolutionalVisionTransformerWithTokenOut(\n",
      "    (stage0): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage1): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage2): VisionTransformer(\n",
      "      (patch_embed): ConvEmbed(\n",
      "        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.2, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (conv_proj_q): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (head): Linear(in_features=384, out_features=192, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionEmbeddingLearned(\n",
      "    (row_embed): Embedding(50, 192)\n",
      "    (col_embed): Embedding(50, 192)\n",
      "  )\n",
      "  (query_encoder): Embedding(50, 384)\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (kd_linear): Linear(in_features=384, out_features=64, bias=True)\n",
      "  (posori_mlp): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (1): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (2): Linear(in_features=384, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-04-12 04:47:56,985:[P:8712]:Rank[0/1] Trainable Model Total Parameter: \t31.9M\n",
      "2025-04-12 04:47:56,985:[P:8712]:Rank[0/1] Trainable Model Total Parameter: \t31.9M\n",
      "2025-04-12 04:47:56,985:[P:8712]:Rank[0/1] Trainable Model Total Parameter: \t31.9M\n",
      "2025-04-12 04:47:56,987:[P:8712]:Rank[0/1] == model_stats by tensorwatch ==\n",
      "2025-04-12 04:47:56,987:[P:8712]:Rank[0/1] == model_stats by tensorwatch ==\n",
      "2025-04-12 04:47:56,987:[P:8712]:Rank[0/1] == model_stats by tensorwatch ==\n",
      "2025-04-12 04:47:57,002:[P:8712]:Rank[0/1] => error when run model_stats\n",
      "2025-04-12 04:47:57,002:[P:8712]:Rank[0/1] => error when run model_stats\n",
      "2025-04-12 04:47:57,002:[P:8712]:Rank[0/1] => error when run model_stats\n",
      "2025-04-12 04:47:57,004:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n",
      "2025-04-12 04:47:57,004:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n",
      "2025-04-12 04:47:57,004:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n",
      "2025-04-12 04:47:57,123:[P:8712]:Rank[0/1] => FLOPs: 13.67 GMac, params: 31.88 M \n",
      "2025-04-12 04:47:57,123:[P:8712]:Rank[0/1] => FLOPs: 13.67 GMac, params: 31.88 M \n",
      "2025-04-12 04:47:57,123:[P:8712]:Rank[0/1] => FLOPs: 13.67 GMac, params: 31.88 M \n",
      "2025-04-12 04:47:57,126:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n",
      "2025-04-12 04:47:57,126:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n",
      "2025-04-12 04:47:57,126:[P:8712]:Rank[0/1] == get_model_complexity_info by ptflops ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module LayerNorm is treated as a zero-op.\n",
      "Warning: module ConvEmbed is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module Rearrange is treated as a zero-op.\n",
      "Warning: module Attention is treated as a zero-op.\n",
      "Warning: module Identity is treated as a zero-op.\n",
      "Warning: module QuickGELU is treated as a zero-op.\n",
      "Warning: module Mlp is treated as a zero-op.\n",
      "Warning: module Block is treated as a zero-op.\n",
      "Warning: module VisionTransformer is treated as a zero-op.\n",
      "Warning: module DropPath is treated as a zero-op.\n",
      "Warning: module ConvolutionalVisionTransformerWithTokenOut is treated as a zero-op.\n",
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module PositionEmbeddingLearned is treated as a zero-op.\n",
      "Warning: module _LinearWithBias is treated as a zero-op.\n",
      "Warning: module LayerNorm is treated as a zero-op.\n",
      "Warning: module TransformerDecoderLayer is treated as a zero-op.\n",
      "Warning: module TransformerDecoder is treated as a zero-op.\n",
      "Warning: module MLP is treated as a zero-op.\n",
      "Warning: module CVTDETR is treated as a zero-op.\n",
      "CVTDETR(\n",
      "  31.812 M, 99.772% Params, 13.668 GMac, 100.000% MACs, \n",
      "  (cvt): ConvolutionalVisionTransformerWithTokenOut(\n",
      "    19.667 M, 61.682% Params, 11.902 GMac, 87.079% MACs, \n",
      "    (stage0): VisionTransformer(\n",
      "      0.061 M, 0.192% Params, 0.493 GMac, 3.610% MACs, \n",
      "      (patch_embed): ConvEmbed(\n",
      "        0.009 M, 0.030% Params, 0.087 GMac, 0.639% MACs, \n",
      "        (proj): Conv2d(0.009 M, 0.030% Params, 0.087 GMac, 0.639% MACs, 3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "        (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        0.052 M, 0.163% Params, 0.406 GMac, 2.971% MACs, \n",
      "        (0): Block(\n",
      "          0.052 M, 0.163% Params, 0.406 GMac, 2.971% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.019 M, 0.059% Params, 0.104 GMac, 0.762% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.001 M, 0.002% Params, 0.006 GMac, 0.047% MACs, \n",
      "              (conv): Conv2d(0.001 M, 0.002% Params, 0.005 GMac, 0.039% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.000% Params, 0.001 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.001 M, 0.002% Params, 0.002 GMac, 0.012% MACs, \n",
      "              (conv): Conv2d(0.001 M, 0.002% Params, 0.001 GMac, 0.010% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.000% Params, 0.0 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.001 M, 0.002% Params, 0.002 GMac, 0.012% MACs, \n",
      "              (conv): Conv2d(0.001 M, 0.002% Params, 0.001 GMac, 0.010% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.000% Params, 0.0 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.004 M, 0.013% Params, 0.038 GMac, 0.276% MACs, in_features=64, out_features=64, bias=True)\n",
      "            (proj_k): Linear(0.004 M, 0.013% Params, 0.009 GMac, 0.069% MACs, in_features=64, out_features=64, bias=True)\n",
      "            (proj_v): Linear(0.004 M, 0.013% Params, 0.009 GMac, 0.069% MACs, in_features=64, out_features=64, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "            (proj): Linear(0.004 M, 0.013% Params, 0.038 GMac, 0.276% MACs, in_features=64, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            0.033 M, 0.104% Params, 0.302 GMac, 2.210% MACs, \n",
      "            (fc1): Linear(0.017 M, 0.052% Params, 0.151 GMac, 1.105% MACs, in_features=64, out_features=256, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.016 M, 0.052% Params, 0.151 GMac, 1.105% MACs, in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage1): VisionTransformer(\n",
      "      1.012 M, 3.173% Params, 2.053 GMac, 15.024% MACs, \n",
      "      (patch_embed): ConvEmbed(\n",
      "        0.111 M, 0.347% Params, 0.255 GMac, 1.868% MACs, \n",
      "        (proj): Conv2d(0.111 M, 0.347% Params, 0.255 GMac, 1.868% MACs, 64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        0.901 M, 2.825% Params, 1.798 GMac, 13.157% MACs, \n",
      "        (0): Block(\n",
      "          0.45 M, 1.413% Params, 0.899 GMac, 6.578% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.155 M, 0.485% Params, 0.22 GMac, 1.607% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.005 GMac, 0.036% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.004 GMac, 0.029% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.006% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.001 GMac, 0.009% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.001 GMac, 0.007% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.002% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.001 GMac, 0.009% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.001 GMac, 0.007% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.002% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.037 M, 0.116% Params, 0.085 GMac, 0.621% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(0.037 M, 0.116% Params, 0.021 GMac, 0.155% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(0.037 M, 0.116% Params, 0.021 GMac, 0.155% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "            (proj): Linear(0.037 M, 0.116% Params, 0.085 GMac, 0.621% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            0.296 M, 0.928% Params, 0.679 GMac, 4.971% MACs, \n",
      "            (fc1): Linear(0.148 M, 0.465% Params, 0.34 GMac, 2.486% MACs, in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.148 M, 0.463% Params, 0.34 GMac, 2.486% MACs, in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          0.45 M, 1.413% Params, 0.899 GMac, 6.578% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.155 M, 0.485% Params, 0.22 GMac, 1.607% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.005 GMac, 0.036% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.004 GMac, 0.029% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.006% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.001 GMac, 0.009% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.001 GMac, 0.007% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.002% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.002 M, 0.007% Params, 0.001 GMac, 0.009% MACs, \n",
      "              (conv): Conv2d(0.002 M, 0.005% Params, 0.001 GMac, 0.007% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "              (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.002% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.037 M, 0.116% Params, 0.085 GMac, 0.621% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_k): Linear(0.037 M, 0.116% Params, 0.021 GMac, 0.155% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_v): Linear(0.037 M, 0.116% Params, 0.021 GMac, 0.155% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "            (proj): Linear(0.037 M, 0.116% Params, 0.085 GMac, 0.621% MACs, in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            0.296 M, 0.928% Params, 0.679 GMac, 4.971% MACs, \n",
      "            (fc1): Linear(0.148 M, 0.465% Params, 0.34 GMac, 2.486% MACs, in_features=192, out_features=768, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.148 M, 0.463% Params, 0.34 GMac, 2.486% MACs, in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (stage2): VisionTransformer(\n",
      "      18.52 M, 58.085% Params, 9.355 GMac, 68.444% MACs, \n",
      "      (patch_embed): ConvEmbed(\n",
      "        0.664 M, 2.082% Params, 0.382 GMac, 2.798% MACs, \n",
      "        (proj): Conv2d(0.664 M, 2.082% Params, 0.382 GMac, 2.798% MACs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        17.856 M, 56.003% Params, 8.972 GMac, 65.646% MACs, \n",
      "        (0): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Block(\n",
      "          1.786 M, 5.600% Params, 0.897 GMac, 6.565% MACs, \n",
      "          (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            0.604 M, 1.894% Params, 0.217 GMac, 1.585% MACs, \n",
      "            (conv_proj_q): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.002 GMac, 0.018% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.002 GMac, 0.015% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_k): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (conv_proj_v): Sequential(\n",
      "              0.004 M, 0.013% Params, 0.001 GMac, 0.004% MACs, \n",
      "              (conv): Conv2d(0.003 M, 0.011% Params, 0.0 GMac, 0.004% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.001% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (rearrage): Rearrange('b c h w -> b (h w) c')\n",
      "            )\n",
      "            (proj_q): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_k): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_v): Linear(0.148 M, 0.464% Params, 0.021 GMac, 0.156% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "            (proj): Linear(0.148 M, 0.464% Params, 0.085 GMac, 0.623% MACs, in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path): DropPath(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "          (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            1.182 M, 3.706% Params, 0.681 GMac, 4.980% MACs, \n",
      "            (fc1): Linear(0.591 M, 1.855% Params, 0.34 GMac, 2.490% MACs, in_features=384, out_features=1536, bias=True)\n",
      "            (act): QuickGELU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "            (fc2): Linear(0.59 M, 1.851% Params, 0.34 GMac, 2.490% MACs, in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "    (head): Linear(0.074 M, 0.232% Params, 0.0 GMac, 0.001% MACs, in_features=384, out_features=192, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionEmbeddingLearned(\n",
      "    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, \n",
      "    (row_embed): Embedding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 50, 192)\n",
      "    (col_embed): Embedding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 50, 192)\n",
      "  )\n",
      "  (query_encoder): Embedding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 50, 384)\n",
      "  (decoder): TransformerDecoder(\n",
      "    11.823 M, 37.082% Params, 1.67 GMac, 12.218% MACs, \n",
      "    (layers): ModuleList(\n",
      "      11.823 M, 37.082% Params, 1.67 GMac, 12.218% MACs, \n",
      "      (0): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        1.971 M, 6.180% Params, 0.278 GMac, 2.036% MACs, \n",
      "        (self_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.032 GMac, 0.231% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          0.591 M, 1.855% Params, 0.207 GMac, 1.518% MACs, \n",
      "          (out_proj): _LinearWithBias(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(0.394 M, 1.236% Params, 0.02 GMac, 0.144% MACs, in_features=384, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (linear2): Linear(0.394 M, 1.234% Params, 0.02 GMac, 0.144% MACs, in_features=1024, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (kd_linear): Linear(0.025 M, 0.077% Params, 0.007 GMac, 0.054% MACs, in_features=384, out_features=64, bias=True)\n",
      "  (posori_mlp): MLP(\n",
      "    0.297 M, 0.931% Params, 0.089 GMac, 0.650% MACs, \n",
      "    (layers): ModuleList(\n",
      "      0.297 M, 0.931% Params, 0.089 GMac, 0.650% MACs, \n",
      "      (0): Linear(0.148 M, 0.464% Params, 0.044 GMac, 0.324% MACs, in_features=384, out_features=384, bias=True)\n",
      "      (1): Linear(0.148 M, 0.464% Params, 0.044 GMac, 0.324% MACs, in_features=384, out_features=384, bias=True)\n",
      "      (2): Linear(0.001 M, 0.004% Params, 0.0 GMac, 0.003% MACs, in_features=384, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Hungarian: \n",
      "w_kd:  1.0\n",
      "w_posori 1.0\n",
      "use ori? True\n",
      "inter losses? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MatchThenSupervise(\n",
       "  (matcher): HungarianMatcher()\n",
       "  (KDTokenLoss): MSELoss()\n",
       "  (KDGlobalLoss): MSELoss()\n",
       "  (PosoriLoss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_distributed(args)\n",
    "setup_cudnn(config)\n",
    "\n",
    "update_config(config, args)\n",
    "final_output_dir = create_logger(config, args.cfg, 'test')\n",
    "tb_log_dir = final_output_dir\n",
    "\n",
    "if comm.is_main_process():\n",
    "    output_config_path = os.path.join(final_output_dir, 'config.yaml')\n",
    "\n",
    "model = build_model(config)\n",
    "\n",
    "model_file = config.TEST.MODEL_FILE if config.TEST.MODEL_FILE \\\n",
    "    else os.path.join(final_output_dir, 'model_best.pth')\n",
    "ext = model_file.split('.')[-1]\n",
    "if ext == 'pth':\n",
    "    state_dict = torch.load(model_file, map_location=\"cpu\")\n",
    "    if \"checkpoint\" in model_file:\n",
    "        state_dict = state_dict['state_dict']\n",
    "else:\n",
    "    raise ValueError(\"Unknown model file\")\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.to(torch.device('cuda'))\n",
    "\n",
    "writer_dict = {\n",
    "    'writer': SummaryWriter(logdir=tb_log_dir),\n",
    "    'train_global_steps': 0,\n",
    "    'valid_global_steps': 0,\n",
    "}\n",
    "\n",
    "summary_model_on_master(model, config, final_output_dir, False)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = build_criterion(config, train=False)\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85534759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# data points in test/val dataset:  100\n"
     ]
    }
   ],
   "source": [
    "valid_loader = build_dataloader(config, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
